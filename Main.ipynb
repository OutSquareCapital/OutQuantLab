{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des modules\n",
    "import Config\n",
    "from Signals import *\n",
    "import Portfolio\n",
    "import Data\n",
    "import Dashboard\n",
    "import Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appels des méthodes d'indicateurs\n",
    "methods = [\n",
    "    #SeasonalBreakout.seasonal_breakout_returns,\n",
    "    #SeasonalBreakoutTrend.seasonal_breakout_returns_trend,\n",
    "\n",
    "    #RiskPremium.fixed_bias,\n",
    "\n",
    "    #Trend.mean_price_ratio,                                             #VALIDE\n",
    "    #Trend.median_price_ratio,                                             #VALIDE\n",
    "    #Trend.central_price_ratio,                                          #VALIDE\n",
    "    #Trend.mean_rate_of_change,                                          #VALIDE\n",
    "    #Trend.median_rate_of_change,                                        #VALIDE\n",
    "    #Trend.central_rate_of_change,                                       #VALIDE                 \n",
    "\n",
    "    #Acceleration.mean_price_macd,                                       #VALIDE\n",
    "    #Acceleration.median_price_macd,                                     #VALIDE\n",
    "    #Acceleration.central_price_macd,                                    #VALIDE\n",
    "    #Acceleration.mean_rate_of_change_macd,                              #VALIDE\n",
    "    #Acceleration.median_rate_of_change_macd,                            #VALIDE\n",
    "    #Acceleration.central_rate_of_change_macd,                           #VALIDE \n",
    "\n",
    "    #AccelerationTrend.mean_price_macd_trend,                            #VALIDE\n",
    "    #AccelerationTrend.median_price_macd_trend,                          #VALIDE\n",
    "    #AccelerationTrend.central_price_macd_trend,                         #VALIDE\n",
    "    #AccelerationTrend.mean_rate_of_change_macd_trend,                   #VALIDE\n",
    "    #AccelerationTrend.median_rate_of_change_macd_trend,                 #VALIDE\n",
    "    #AccelerationTrend.central_rate_of_change_macd_trend,                #VALIDE \n",
    "\n",
    "    #MeanReversion.mean_price_ratio_normalised,                             #VALIDE \n",
    "    #MeanReversion.mean_rate_of_change_normalised,                          #VALIDE \n",
    "\n",
    "    #MeanReversionTrend.mean_price_ratio_normalised_trend,                  #VALIDE  \n",
    "    #MeanReversionTrend.mean_rate_of_change_normalised_trend,               #VALIDE \n",
    "\n",
    "    #ReturnsDistribution.skewness,                                         #VALIDE, WORK BEST LT\n",
    "    #ReturnsDistribution.relative_skewness,                                 #VALIDE, WORK BEST ST\n",
    "    #ReturnsDistribution.skewness_on_kurtosis_ST,                           #VALIDE, WORK BEST ST\n",
    "    #ReturnsDistribution.skewness_on_kurtosis_LT,                           #VALIDE, WORK BEST LT\n",
    "    #ReturnsDistribution.relative_skewness_on_kurtosis_ST,                  #VALIDE, WORK BEST ST\n",
    "    #ReturnsDistribution.relative_skewness_on_kurtosis_LT,                  #VALIDE, WORK BEST LT\n",
    "\n",
    "    #ReturnsDistributionTrend.skewness_trend,                               #VALIDE\n",
    "    #ReturnsDistributionTrend.relative_skewness_trend,                      #VALIDE\n",
    "    #ReturnsDistributionTrend.skewness_on_kurtosis_ST_trend,                #VALIDE\n",
    "    #ReturnsDistributionTrend.skewness_on_kurtosis_LT_trend,                #VALIDE   \n",
    "    #ReturnsDistributionTrend.relative_skewness_on_kurtosis_ST_trend,       #VALIDE   \n",
    "    #ReturnsDistributionTrend.relative_skewness_on_kurtosis_LT_trend,       #VALIDE \n",
    "    \n",
    "    #Volatility.relative_directional_volatility,                            #VALIDE, WORK BEST LT\n",
    "    #Volatility.normalised_directional_volatility,                          #VALIDE, WORK BEST ST\n",
    "\n",
    "    #VolatilityTrend.relative_directional_volatility_trend,                 #VALIDE\n",
    "    #VolatilityTrend.normalised_directional_volatility_trend,               #VALIDE\n",
    "\n",
    "    #Seasonality.seasonal_trend,\n",
    "\n",
    "    #SeasonalityTrend.overall_seasonal_trend\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification des options de paramètres\n",
    "\n",
    "param_options = {\n",
    "    'Trend': {\n",
    "        'LenST': Backtest.param_range_values(4, 64, 5),\n",
    "        'LenLT': Backtest.param_range_values(16, 256, 5),\n",
    "    },\n",
    "    'Acceleration': {\n",
    "        'LenST': Backtest.param_range_values(16, 64, 3),\n",
    "        'LenLT': Backtest.param_range_values(64, 256, 3),\n",
    "        'MacdLength': Backtest.param_range_values(2, 64, 6),\n",
    "    },\n",
    "    'AccelerationTrend': {\n",
    "        'LenST': Backtest.param_range_values(16, 64, 3),\n",
    "        'LenLT': Backtest.param_range_values(64, 256, 3),\n",
    "        'MacdLength': Backtest.param_range_values(2, 64, 3),\n",
    "        'TrendLenST': Backtest.param_range_values(4, 64, 5),\n",
    "        'TrendLenLT': Backtest.param_range_values(16, 256, 5),\n",
    "    },\n",
    "    'MeanReversion': {\n",
    "        'SignalLength': Backtest.param_range_values(4, 64, 5),\n",
    "        'PLength': Backtest.param_range_values(128, 2048, 5),\n",
    "    },\n",
    "    'MeanReversionTrend': {\n",
    "        'SignalLength': Backtest.param_range_values(4, 64, 5),\n",
    "        'PLength': Backtest.param_range_values(128, 2048, 5),\n",
    "        'LenST': Backtest.param_range_values(16, 64, 3),\n",
    "        'LenLT': Backtest.param_range_values(64, 256, 3),\n",
    "    },\n",
    "    'ReturnsDistribution': {\n",
    "        'LenSmooth': Backtest.param_range_values(1, 1, 1),\n",
    "        'LenSkew': Backtest.param_range_values(4, 512, 8),\n",
    "    },\n",
    "    'ReturnsDistributionTrend': {\n",
    "        'LenSmooth': Backtest.param_range_values(1, 32, 6),\n",
    "        'LenSkew': Backtest.param_range_values(4, 512, 6),\n",
    "        'TrendLenST': Backtest.param_range_values(4, 64, 5),\n",
    "        'TrendLenLT': Backtest.param_range_values(16, 256, 5),\n",
    "    },\n",
    "    'Volatility': {\n",
    "        'LenVol': Backtest.param_range_values(4, 256, 7),\n",
    "        'LenST': Backtest.param_range_values(1, 64, 7),\n",
    "        'LenLT': Backtest.param_range_values(64, 512, 4),\n",
    "    },\n",
    "    'VolatilityTrend': {\n",
    "        'LenVol': Backtest.param_range_values(4, 256, 7),\n",
    "        'LenST': Backtest.param_range_values(1, 1, 5),\n",
    "        'LenLT': Backtest.param_range_values(64, 512, 4),\n",
    "        'TrendLenST': Backtest.param_range_values(4, 64, 5),\n",
    "        'TrendLenLT': Backtest.param_range_values(16, 256, 5),\n",
    "    },\n",
    "    'Seasonality': {\n",
    "        'GroupBy': [1],\n",
    "        'GroupSelected': [1,2,3,4,5],\n",
    "        'LenST': Backtest.param_range_values(1, 64, 6),\n",
    "        'LenLT': Backtest.param_range_values(4, 256, 6),\n",
    "    },\n",
    "    'SeasonalityTrend': {\n",
    "        'GroupBy': [1],\n",
    "        'GroupSelected': [1],\n",
    "         'LenST': Backtest.param_range_values(1, 64, 1),\n",
    "        'LenLT': Backtest.param_range_values(4, 256, 1),\n",
    "        'TrendLenST': Backtest.param_range_values(4, 64, 5),\n",
    "        'TrendLenLT': Backtest.param_range_values(16, 256, 5),\n",
    "    },\n",
    "    'RiskPremium': {\n",
    "        'Bias': [1]\n",
    "    },\n",
    "\n",
    "\n",
    "  'SeasonalBreakout': {\n",
    "        'LengthMean': Backtest.param_range_values(64, 256, 1),\n",
    "        'LengthSnapshot': Backtest.param_range_values(2, 16, 1),\n",
    "        'amplitude': Backtest.param_range_values(10, 20, 1, linear = True),\n",
    "    },\n",
    "    'SeasonalBreakoutTrend': {\n",
    "        'LengthMean': Backtest.param_range_values(64, 256, 1),\n",
    "        'LengthSnapshot': Backtest.param_range_values(2, 16, 1),\n",
    "        'amplitude': Backtest.param_range_values(10, 20, 1, linear = True),\n",
    "        'LenST': Backtest.param_range_values(16, 64, 1),\n",
    "        'LenLT': Backtest.param_range_values(64, 256, 1),\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération automatisée des indicateurs et params\n",
    "indicators_and_params = Backtest.automatic_generation(methods, param_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting & cleaning\n",
    "\n",
    "#Data.get_yahoo_finance_data(Config.yahoo_assets, Config.file_path_yf)\n",
    "\n",
    "#data_prices_df, _ = Data.load_prices_from_csv(Config.file_path_tradingview, dtype=np.float64)\n",
    "\n",
    "#Data.clean_and_process_prices(Config.file_path_tradingview, ['VX'])\n",
    "\n",
    "#data_prices_df_yahoo, _ = LoadInternalData.load_prices_from_csv(Config.file_path_yf, dtype=np.float64)\n",
    "\n",
    "#CleanData.clean_and_process_prices(Config.file_path_yf, ['VIXY'])\n",
    "\n",
    "#data_prices_df_merged = MergeData.raccommoder_prices_futures_etf(data_prices_df, data_prices_df_yahoo, paires_futures_etf)\n",
    "\n",
    "#data_prices_df_merged.to_csv(file_path_main_data)\n",
    "\n",
    "#CleanData.clean_and_process_prices(file_path_main_data, ['VX'])\n",
    "\n",
    "#test = CleanData.adjust_prices_with_risk_free_rate(etf_data, bill_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 1. Charger tous les fichiers CSV dans un seul DataFrame\n",
    "def load_contracts_data(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and combine the 'Date', 'Close', 'Volume', and 'OpenInt' columns from all CSV files in a folder\n",
    "    into a single DataFrame, with the contract name extracted from the filename.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        Path to the folder containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined DataFrame with data from all contracts, indexed by 'Date'.\n",
    "    \"\"\"\n",
    "    files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "    data_lst = []\n",
    "\n",
    "    for file in files:\n",
    "        # Extract contract name from filename\n",
    "        contract = os.path.basename(file).replace(\".csv\", \"\")\n",
    "\n",
    "        # Load only the required columns from the CSV file\n",
    "        df = pd.read_csv(file, usecols=[\"Date\", \"Close\", \"Volume\", \"OpenInt\"], parse_dates=[\"Date\"], dayfirst=False)\n",
    "        \n",
    "        # Add contract name as a new column\n",
    "        df[\"Contract\"] = contract\n",
    "        data_lst.append(df)\n",
    "\n",
    "    # Concatenate all data into a single DataFrame and set 'Date' as the index\n",
    "    contracts_df = pd.concat(data_lst, ignore_index=True).set_index(\"Date\")\n",
    "\n",
    "    return contracts_df\n",
    "\n",
    "\n",
    "# 2. Calculer la MA10 du volume pour chaque contrat\n",
    "def calculate_moving_average_volume(contracts_df, lookback=5):\n",
    "    \"\"\"\n",
    "    Calcule la moyenne mobile du volume pour chaque contrat.\n",
    "    \n",
    "    Paramètres:\n",
    "    - contracts_df: DataFrame contenant les données de tous les contrats\n",
    "\n",
    "    Retour:\n",
    "    - contracts_df avec la colonne ajoutée 'Mean_Volume'\n",
    "    \"\"\"\n",
    "    contracts_df[\"Mean_OI\"] = contracts_df.groupby(\"Contract\")[\"OpenInt\"].transform(lambda x: x.rolling(lookback, min_periods=1).mean())\n",
    "    contracts_df[\"Mean_Volume\"] = contracts_df.groupby(\"Contract\")[\"Volume\"].transform(lambda x: x.rolling(lookback, min_periods=1).mean())\n",
    "\n",
    "    return contracts_df\n",
    "\n",
    "\n",
    "def create_continuous_price_series(prices_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a continuous series of prices based on the contract with the highest mean volume each day.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    prices_df : pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "            - 'Date' (datetime): The date of each observation.\n",
    "            - 'Close' (float): Closing price of each contract.\n",
    "            - 'Mean_Volume' (float): Moving average of contract volume.\n",
    "            - 'Contract' (str): Contract identifier.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with 'Date' as index, continuous 'Close' prices, and 'Contract_Change' column indicating changes.\n",
    "    \"\"\"\n",
    "    # Sort by 'Date' and 'Mean_Volume' to select contracts with the highest average volume per day\n",
    "    prices_df_sorted = prices_df.sort_values(['Date', 'Mean_OI', 'Mean_Volume'], ascending=[True, False, False])\n",
    "\n",
    "    # Group by 'Date' to get the contract with the highest Mean_Volume each day\n",
    "    highest_liquidity_df = prices_df_sorted.groupby('Date').first().reset_index()\n",
    "\n",
    "    # Identify contract changes\n",
    "    highest_liquidity_df['Contract_Change'] = highest_liquidity_df['Contract'] != highest_liquidity_df['Contract'].shift()\n",
    "\n",
    "    return highest_liquidity_df[['Date', 'Close', 'Contract', 'Contract_Change']].set_index('Date')\n",
    "\n",
    "\n",
    "def backadjust_price_series(unadjusted_df: pd.DataFrame, use_price_diff: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applique un ajustement en arrière des prix de futures pour maintenir la continuité historique.\n",
    "    Utilise soit un ajustement par différence de prix, soit un ajustement par ratio, en appliquant\n",
    "    les ajustements en sens inverse du temps.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    unadjusted_df : pd.DataFrame\n",
    "        DataFrame des prix non ajustés avec les jours de roll identifiés (index en datetime, Close, Contract_Change).\n",
    "    use_price_diff : bool, optional\n",
    "        Choisit le mode d'ajustement : True pour ajustement par différence de prix, False pour ajustement par ratio (par défaut False).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec 'Date' comme index et les prix ajustés en continuité.\n",
    "    \"\"\"\n",
    "    cumulative_adjustment = 0.0 if use_price_diff else 1.0\n",
    "\n",
    "    adjusted_close_lst = unadjusted_df['Close'].copy()\n",
    "\n",
    "    # Parcours du DataFrame de bas en haut pour appliquer l'ajustement en arrière\n",
    "    for i in range(len(unadjusted_df) - 1, 0, -1):\n",
    "        if unadjusted_df['Contract_Change'].iloc[i] == True:\n",
    "            # Calcul du ratio ou de la différence entre le contrat actuel et le précédent\n",
    "            next_close = unadjusted_df['Close'].iloc[i]\n",
    "            previous_close = unadjusted_df['Close'].iloc[i - 1]\n",
    "            \n",
    "            if use_price_diff:\n",
    "                # Calcul de l'ajustement en arrière par différence\n",
    "                price_diff = next_close - previous_close\n",
    "                cumulative_adjustment += price_diff\n",
    "            else:\n",
    "                # Calcul de l'ajustement en arrière par ratio\n",
    "                price_ratio = next_close / previous_close\n",
    "                cumulative_adjustment *= price_ratio\n",
    "\n",
    "        # Appliquer l'ajustement cumulatif aux prix antérieurs\n",
    "        adjusted_close_lst.iloc[i - 1] = (adjusted_close_lst.iloc[i - 1] + cumulative_adjustment \n",
    "                                          if use_price_diff \n",
    "                                          else adjusted_close_lst.iloc[i - 1] * cumulative_adjustment)\n",
    "\n",
    "    # Mise à jour du DataFrame avec les prix ajustés\n",
    "    unadjusted_df['Close'] = adjusted_close_lst\n",
    "\n",
    "    # Renvoie uniquement la colonne des prix ajustés avec 'Date' comme index\n",
    "    final_adjusted_df = unadjusted_df[['Close']]\n",
    "\n",
    "    return final_adjusted_df\n",
    "\n",
    "\n",
    "def identify_roll_dates(adjusted_df: pd.DataFrame, unadjusted_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifie les jours de roll en calculant la variation en pourcentage entre les prix ajustés et non ajustés,\n",
    "    et considère un jour de roll lorsque la variation dépasse 0.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    adjusted_df : pd.DataFrame\n",
    "        DataFrame des prix ajustés (Date, Close).\n",
    "    unadjusted_df : pd.DataFrame\n",
    "        DataFrame des prix non ajustés (Date, Close).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame original des prix non ajustés avec la colonne 'Roll_Day' ajoutée.\n",
    "    \"\"\"\n",
    "    # Calculer la variation en pourcentage entre les prix ajustés et non ajustés\n",
    "    unadjusted_df['Price_Diff'] = adjusted_df['Close'] - unadjusted_df['Close']\n",
    "\n",
    "    unadjusted_df['Price_Diff_Change'] = unadjusted_df['Price_Diff'].pct_change().round(2)\n",
    "\n",
    "    # Identifier les jours de roll (quand la variation en pourcentage dépasse le seuil)\n",
    "    unadjusted_df['Roll_Day'] = (unadjusted_df['Price_Diff_Change'].abs() > 0).astype(int)\n",
    "    \n",
    "    # Supprimer la colonne temporaire 'Price_Diff'\n",
    "    #unadjusted_df.drop(columns=['Price_Diff'], inplace=True)\n",
    "    \n",
    "    return unadjusted_df\n",
    "\n",
    "\n",
    "def apply_adjustment(unadjusted_df: pd.DataFrame, use_price_diff: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applique un ajustement en arrière des prix de futures pour maintenir la continuité historique.\n",
    "    Utilise soit un ajustement par différence de prix, soit un ajustement par ratio.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    unadjusted_df : pd.DataFrame\n",
    "        DataFrame des prix non ajustés avec les jours de roll identifiés (index en datetime, Close, Roll_Day).\n",
    "    use_price_diff : bool, optional\n",
    "        Choisit le mode d'ajustement : True pour ajustement par différence de prix, False pour ajustement par ratio (par défaut False).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame avec 'Date' comme index et les prix ajustés en continuité.\n",
    "    \"\"\"\n",
    "    cumulative_adjustment = 0.0 if use_price_diff else 1.0\n",
    "\n",
    "    adjusted_close_lst = unadjusted_df['Close'].copy()\n",
    "\n",
    "    # Parcours du DataFrame de bas en haut pour appliquer l'ajustement en arrière\n",
    "    for i in range(len(unadjusted_df) - 1, 0, -1):\n",
    "        if unadjusted_df['Roll_Day'].iloc[i] == 1:\n",
    "            # Calcul du ratio ou de la différence entre le contrat actuel et le précédent\n",
    "            next_close = unadjusted_df['Close'].iloc[i]\n",
    "            previous_close = unadjusted_df['Close'].iloc[i - 1]\n",
    "            \n",
    "            if use_price_diff:\n",
    "                # Calcul de l'ajustement en arrière par différence\n",
    "                price_diff = next_close - previous_close\n",
    "                cumulative_adjustment += price_diff\n",
    "            else:\n",
    "                # Calcul de l'ajustement en arrière par ratio\n",
    "                price_ratio = next_close / previous_close\n",
    "                cumulative_adjustment *= price_ratio\n",
    "\n",
    "        # Appliquer l'ajustement cumulatif aux prix antérieurs\n",
    "        adjusted_close_lst.iloc[i - 1] = (adjusted_close_lst.iloc[i - 1] + cumulative_adjustment \n",
    "                                          if use_price_diff \n",
    "                                          else adjusted_close_lst.iloc[i - 1] * cumulative_adjustment)\n",
    "\n",
    "    # Mise à jour du DataFrame avec les prix ajustés\n",
    "    unadjusted_df['Close'] = adjusted_close_lst\n",
    "\n",
    "    # Renvoie uniquement la colonne des prix ajustés avec 'Date' comme index\n",
    "    final_adjusted_df = unadjusted_df[['Close']]\n",
    "\n",
    "    return final_adjusted_df\n",
    "\n",
    "adjusted_front_df = pd.read_csv(\n",
    "    \"D:\\\\Python\\\\Data\\\\Excel_Data\\\\TradingView\\\\AdjustedFrontMonth\\\\VX.csv\",\n",
    "    usecols=['time', 'close'],\n",
    "    parse_dates=['time']\n",
    ").rename(columns={'time': 'Date', 'close': 'Close'}).set_index('Date')\n",
    "\n",
    "unadjusted_front_df = pd.read_csv(\n",
    "    \"D:\\\\Python\\\\Data\\\\Excel_Data\\\\TradingView\\\\UnadjustedFrontMonth\\\\VX.csv\",\n",
    "    usecols=['time', 'close'],\n",
    "    parse_dates=['time']\n",
    ").rename(columns={'time': 'Date', 'close': 'Close'}).set_index('Date')\n",
    "\n",
    "# Identification des jours de roll\n",
    "roll_dates_df = identify_roll_dates(adjusted_front_df, unadjusted_front_df)\n",
    "\n",
    "# Ajustement des prix\n",
    "Final_adjusted_prices_df = apply_adjustment(roll_dates_df, use_price_diff=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prices_df, assets_names = Data.load_prices_from_csv(Config.file_path_yf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification des actifs à Backtest\n",
    "assets = assets_names\n",
    "ratios = []\n",
    "ensembles = []\n",
    "canary_assets = []\n",
    "canary_ratios = []\n",
    "canary_ensembles = []\n",
    "\n",
    "data = Data.process_category_data(\n",
    "    assets_names, \n",
    "    data_prices_df, \n",
    "    assets, \n",
    "    ratios, \n",
    "    ensembles, \n",
    "    canary_assets, \n",
    "    canary_ratios, \n",
    "    canary_ensembles,\n",
    "    initial_equity=100\n",
    ")\n",
    "\n",
    "prices_df_test = data['returnstreams']['prices_df']\n",
    "pct_returns_df_test = data['returnstreams']['pct_returns_df']\n",
    "log_returns_df_test = data['returnstreams']['log_returns_df']\n",
    "prices_array_test = data['returnstreams']['prices_array']\n",
    "pct_returns_array_test = data['returnstreams']['pct_returns_array']\n",
    "log_returns_array_test = data['returnstreams']['log_returns_array']\n",
    "hv_array_test = data['returnstreams']['hv_array']\n",
    "asset_names_test = data['returnstreams']['asset_names']\n",
    "\n",
    "portfolio_base_structure = Portfolio.classify_assets(asset_names_test, Config.portfolio_etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des objets désormais inutiles\n",
    "del (\n",
    "    assets,\n",
    "    data_prices_df, \n",
    "    assets_names, \n",
    "    data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df, raw_adjusted_returns_df = Backtest.SignalsProcessing.trading_signals(prices_array_test,\n",
    "                                                                                log_returns_array_test,\n",
    "                                                                                log_returns_df_test,\n",
    "                                                                                pct_returns_array_test,\n",
    "                                                                                hv_array_test,\n",
    "                                                                                asset_names_test,\n",
    "                                                                                indicators_and_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des objets désormais inutiles\n",
    "del (\n",
    "    indicators_and_params, \n",
    "    prices_df_test, \n",
    "    log_returns_df_test,\n",
    "    prices_array_test, \n",
    "    pct_returns_array_test, \n",
    "    log_returns_array_test, \n",
    "    hv_array_test, \n",
    "    asset_names_test, \n",
    "    signals_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Validation + Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relativized_sharpes_df = RollingSharpe.relative_sharpe_on_confidence_period(raw_adjusted_returns_df, sharpe_lookback = 5000, confidence_lookback=2500)\n",
    "#optimized_returns_rltv = raw_adjusted_returns_df * relativized_sharpes_df.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "def extract_asset_groups(portfolio_dict):\n",
    "    \"\"\"\n",
    "    Fonction récursive pour extraire toutes les listes d'actifs d'un dictionnaire imbriqué.\n",
    "    \n",
    "    Parameters:\n",
    "    - portfolio_dict: Dictionnaire définissant les groupes et sous-groupes d'actifs.\n",
    "    \n",
    "    Returns:\n",
    "    - asset_groups: Liste de toutes les listes d'actifs trouvées dans le dictionnaire.\n",
    "    \"\"\"\n",
    "    asset_groups = []\n",
    "    \n",
    "    def recursive_extract(d):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                recursive_extract(value)  # Continuer à explorer si c'est un sous-dictionnaire\n",
    "            elif isinstance(value, list):\n",
    "                asset_groups.append(value)  # Ajouter la liste d'actifs trouvée\n",
    "            else:\n",
    "                raise ValueError(\"Structure inattendue dans le dictionnaire.\")\n",
    "    \n",
    "    recursive_extract(portfolio_dict)\n",
    "    \n",
    "    return asset_groups\n",
    "\n",
    "\n",
    "def compute_group_diversification_multiplier(group_returns, weights, window):\n",
    "    \"\"\"\n",
    "    Calcule le multiplicateur de diversification pour un groupe d'actifs sur une base roulante.\n",
    "\n",
    "    Parameters:\n",
    "    - group_returns: DataFrame des rendements du groupe d'actifs.\n",
    "    - weights: Array de poids pour les actifs du groupe.\n",
    "    - window: taille de la fenêtre pour la corrélation roulante.\n",
    "\n",
    "    Returns:\n",
    "    - diversification_multiplier_series: Série des multiplicateurs de diversification pour chaque date.\n",
    "    \"\"\"\n",
    "    # Calcul de la matrice de corrélation roulante\n",
    "    rolling_corr = group_returns.rolling(window=window).corr(pairwise=True)\n",
    "\n",
    "    # Calcul du multiplicateur de diversification\n",
    "    num_assets = len(weights)\n",
    "    diversification_multipliers = []\n",
    "\n",
    "    for i in range(window - 1, len(group_returns)):\n",
    "        # Extraire la sous-matrice de corrélation pour chaque fenêtre\n",
    "        corr_matrix = rolling_corr.iloc[i * num_assets: (i + 1) * num_assets].values.reshape(num_assets, num_assets)\n",
    "        \n",
    "        # Calcul de la variance pondérée et du multiplicateur pour cette fenêtre\n",
    "        weighted_variance = weights @ corr_matrix @ weights\n",
    "        diversification_multiplier = 1.0 / np.sqrt(weighted_variance) if weighted_variance > 0 else 1.0\n",
    "        diversification_multipliers.append(diversification_multiplier)\n",
    "\n",
    "    # Convertir en série en alignant les index avec les dates de rolling_corr\n",
    "    diversification_multiplier_series = pd.Series([np.nan] * (window - 1) + diversification_multipliers, \n",
    "                                                  index=group_returns.index, dtype=np.float32)\n",
    "\n",
    "    return diversification_multiplier_series\n",
    "\n",
    "\n",
    "\n",
    "def compute_diversification_for_group(group_assets, returns_df, window):\n",
    "    \"\"\"\n",
    "    Calcule la série de multiplicateurs de diversification pour un groupe spécifique d'actifs.\n",
    "    \n",
    "    Parameters:\n",
    "    - group_assets: Liste des actifs dans le groupe.\n",
    "    - returns_df: DataFrame des rendements pour tous les actifs.\n",
    "    - window: Taille de la fenêtre pour la corrélation roulante.\n",
    "    \n",
    "    Returns:\n",
    "    - Series des multiplicateurs de diversification avec index aligné sur returns_df.\n",
    "    - Liste d'actifs du groupe pour assignation directe.\n",
    "    \"\"\"\n",
    "    group_returns = returns_df[group_assets]\n",
    "    weights = np.full(len(group_assets), 1.0 / len(group_assets), dtype=np.float32)\n",
    "\n",
    "    diversification_multiplier_series = compute_group_diversification_multiplier(\n",
    "        group_returns, weights, window\n",
    "    )\n",
    "    \n",
    "    return diversification_multiplier_series, group_assets\n",
    "\n",
    "def diversification_multiplier_by_group(returns_df, portfolio_dict, window):\n",
    "    \"\"\"\n",
    "    Calcule le multiplicateur de diversification pour chaque actif dans un DataFrame de rendements,\n",
    "    en utilisant les listes d'actifs définies dans le dictionnaire, sur une base roulante, avec parallélisation.\n",
    "    \n",
    "    Parameters:\n",
    "    - returns_df: DataFrame de rendements, chaque colonne correspondant à un actif.\n",
    "    - portfolio_dict: Dictionnaire définissant les groupes et sous-groupes d'actifs.\n",
    "    - window: Taille de la fenêtre pour la corrélation roulante.\n",
    "    \n",
    "    Returns:\n",
    "    - diversification_multiplier_df: DataFrame des multiplicateurs de diversification.\n",
    "    \"\"\"\n",
    "    # Extraire toutes les listes d'actifs depuis le dictionnaire\n",
    "    asset_groups = extract_asset_groups(portfolio_dict)\n",
    "    \n",
    "    # Filtrer les groupes pour ne garder que ceux avec plus d'un actif dans returns_df\n",
    "    valid_groups = [group for group in asset_groups if len([asset for asset in group if asset in returns_df.columns]) > 1]\n",
    "\n",
    "    # Initialiser le DataFrame pour les multiplicateurs de diversification\n",
    "    diversification_multiplier_df = pd.DataFrame(1.0, index=returns_df.index, columns=returns_df.columns, dtype=np.float32)\n",
    "\n",
    "    # Calcul parallèle pour chaque groupe valide\n",
    "    results = Parallel(n_jobs=-1)(delayed(compute_diversification_for_group)(\n",
    "        [asset for asset in group if asset in returns_df.columns], \n",
    "        returns_df, \n",
    "        window\n",
    "    ) for group in valid_groups)\n",
    "\n",
    "    # Combiner les résultats dans le DataFrame final\n",
    "    for diversification_multiplier_series, group_assets in results:\n",
    "        for asset in group_assets:\n",
    "            diversification_multiplier_df.loc[diversification_multiplier_series.index, asset] = diversification_multiplier_series\n",
    "\n",
    "    # Remplacer tous les NaN restants par 1.0\n",
    "    diversification_multiplier_df.fillna(1.0, inplace=True)\n",
    "\n",
    "    diversification_multiplier_df = diversification_multiplier_df.rolling(window=250, min_periods=1).mean()\n",
    "    \n",
    "    return diversification_multiplier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Portfolio.calculate_daily_average_returns(raw_adjusted_returns_df, by_param=False, by_method=True, by_class=False, by_asset=False, global_avg=False)\n",
    "\n",
    "#test_strategy_returns = EqualWeight.calculate_daily_average_returns(raw_adjusted_returns_df, by_method=True, by_class=True, by_asset=True)\n",
    "#test_asset_returns = HierarchicalWeights.generate_recursive_strategy_means(test_strategy_returns, portfolio_strategies)\n",
    "#test_diversification_multipliers = diversification_multiplier_by_group(test_asset_returns,portfolio_base_structure, 2500)\n",
    "#test_global_returns = HierarchicalWeights.generate_recursive_means(test_asset_returns, portfolio_base_structure)\n",
    "#test_global_returns = test_global_returns.rename(columns={test_global_returns.columns[0]: 'sharpe_cluster_optimized'})\n",
    "\n",
    "#benchmark_asset_strategy_returns = EqualWeight.calculate_daily_average_returns(optimized_returns_rltv, by_method=True, by_class=True, by_asset=True)\n",
    "#benchmark_asset_returns = HierarchicalWeights.generate_recursive_strategy_means(benchmark_asset_strategy_returns, portfolio_strategies)\n",
    "#benchmark_global_returns = HierarchicalWeights.generate_recursive_means(benchmark_asset_returns, portfolio_base_structure)\n",
    "#benchmark_global_returns = benchmark_global_returns.rename(columns={benchmark_global_returns.columns[0]: 'cluster_optimized'})\n",
    "\n",
    "#equal_weights_global_returns = EqualWeight.calculate_daily_average_returns(raw_adjusted_returns_df, global_avg=True)\n",
    "#equal_weights_global_returns = equal_weights_global_returns.rename(columns={equal_weights_global_returns.columns[0]: 'equal_weights'})\n",
    "\n",
    "# Concaténation des DataFrames\n",
    "#total_df = pd.concat([benchmark_global_returns, test_global_returns, equal_weights_global_returns], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# FONCTIONNE, PLUS QU'A METTRE EN PLACE AJUSTEMENT POUR TAILLE DE CONTRAT ET RENVOI PAR LISTE ET ON EST BONS\n",
    "def calculate_shares_to_trade_from_adjusted_return(portfolio_size, asset_price, adjusted_return):\n",
    "    \"\"\"\n",
    "    Calcule le nombre d'actions à acheter ou vendre en fonction du rendement ajusté de l'asset.\n",
    "\n",
    "    Args:\n",
    "        portfolio_size (float): La taille totale du portefeuille en $.\n",
    "        asset_price (float): Le prix actuel de l'asset en $.\n",
    "        adjusted_return (float): Le rendement en % ajusté (incluant l'allocation d'actif et la taille du signal).\n",
    "\n",
    "    Returns:\n",
    "        float: Nombre d'actions à acheter (positif) ou vendre (négatif).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcul du montant total à investir en fonction du rendement ajusté\n",
    "    amount_to_invest = portfolio_size * (adjusted_return * 100)\n",
    "\n",
    "    # Calcul du nombre d'actions à acheter ou vendre\n",
    "    shares_to_trade = amount_to_invest / asset_price\n",
    "\n",
    "    return round(shares_to_trade)\n",
    "\n",
    "portfolio_size = 371369\n",
    "asset_price = 6307\n",
    "adjusted_return = -0.000928 # Rendement ajusté en %\n",
    "\n",
    "# Calcul du nombre d'actions à acheter ou vendre\n",
    "shares_to_trade = calculate_shares_to_trade_from_adjusted_return(portfolio_size, asset_price, adjusted_return)\n",
    "print(f\"Position optimale : {shares_to_trade:.2f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_returns = test.loc['2015-01-01':]\n",
    "\n",
    "original_equity_curves = Data.equity_curves_calculs(test_returns, initial_equity = 100000)\n",
    "\n",
    "Dashboard.equity(original_equity_curves)\n",
    "#Dashboard.plot_final_equity_values(test_returns)\n",
    "#Dashboard.drawdowns(original_equity_curves)\n",
    "#Dashboard.max_drawdowns(test_returns)\n",
    "Dashboard.volatility(test_returns, means=False)\n",
    "#Dashboard.rolling_sharpe_ratio(test_returns, window_size=1250)\n",
    "#sharpe_ratios = Dashboard.overall_sharpe_ratios(test_returns)\n",
    "#Dashboard.overall_monthly_skew(test_returns)\n",
    "#Dashboard.sharpe_ratios_heatmap(raw_adjusted_returns_df, 'LenVol', 'LenSmooth')\n",
    "#Dashboard.sharpe_ratios_3d_scatter_plot(raw_adjusted_returns_df, ['LenST', 'LenLT', 'MacdLength'])\n",
    "#Dashboard.params_relative_impact_on_sharpe(daily_returns, ['LenST', 'LenLT'])\n",
    "#Dashboard.sharpe_ratios_yearly_table(combined_df)\n",
    "#Dashboard.correlation_heatmap(test_returns)\n",
    "#Dashboard.average_correlation_bar_chart(test_returns)\n",
    "#Dashboard.sharpe_correlation_ratio_bar_chart(daily_returns)\n",
    "#Dashboard.returns_distribution(daily_returns)\n",
    "#Dashboard.plot_static_clusters(test, 4, 1, 1)\n",
    "#clusters_dict\n",
    "#sharpe_ratios\n",
    "#nan_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
